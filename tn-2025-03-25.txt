import tensorflow as tf
from transformers import TFCLIPModel, CLIPProcessor
from collections import defaultdict
import tensorflow_datasets as tfds
import sys
import numpy as np
from PIL import Image

dataset = tfds.load("tf_flowers", split=["train"], as_supervised=True)[0]
label_names = ['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses']
prompts = [f"a photo of a {label}" for label in label_names]

model = TFCLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

all_test_samples = list(tfds.as_numpy(dataset))
test_samples = all_test_samples[1000:1100]

test_images = [img for img, lbl in test_samples]
test_labels = [label_names[lbl] for img, lbl in test_samples]

# Prepare text features (frozen text encoder)
text_inputs = processor(text=label_names, return_tensors="tf", padding=True)
text_features = model.get_text_features(**text_inputs)
text_features = tf.nn.l2_normalize(text_features, axis=-1)

# Inference
correct = 0
for img, true_label in zip(test_images, test_labels):
    inputs = processor(images=img, return_tensors="tf", padding=True)
    image_features = model.get_image_features(**inputs)
    image_features = tf.nn.l2_normalize(image_features, axis=-1)
    sims = tf.matmul(image_features, text_features, transpose_b=True)
    pred_idx = tf.argmax(sims, axis=1).numpy()[0]
    pred_label = label_names[pred_idx]
    if pred_label == true_label:
        correct += 1

print(f"\nAccuracy on test set1: {correct}/{len(test_images)} = {correct/len(test_images):.2%}")

# Run a forward pass to build weights
dummy_image = Image.fromarray(np.random.randint(0, 256, (224, 224, 3), dtype=np.uint8))
inputs = processor(images=dummy_image, text=["a flower"], return_tensors="tf", padding=True)
_ = model(**inputs)

# Step 2: Fully disable training on all variables
def freeze_all_layers(model):
    for var in model.variables:
        var._trainable = False

freeze_all_layers(model)

# Step 4: Unfreeze ONLY the first LayerNorm in the first vision transformer block
first_block = model.clip.vision_model.encoder.layers[0]
unfrozen = 0
for layer in first_block.submodules:
    if isinstance(layer, tf.keras.layers.LayerNormalization):
        for var in layer.variables:
            var._trainable = True
            print(f"‚úÖ Unfroze: {var.name}")
        unfrozen += 1
        break  # Stop after first LayerNorm

if unfrozen == 0:
    print("‚ùå No LayerNorm was unfrozen!")

# Step 5: Confirm only those variables are trainable
trainable_vars = [v for v in model.variables if v.trainable]
print(f"\n‚úÖ Total trainable variables: {len(trainable_vars)}")
for v in trainable_vars:
    print(v.name, v.shape)

# sys.exit()
# Prepare 16 samples per class from training set
samples_per_class = 100
class_counter = defaultdict(int)
train_images, train_labels = [], []

for image, label in tfds.as_numpy(dataset):
    if class_counter[label] < samples_per_class:
        train_images.append(image)
        train_labels.append(label_names[label])
        class_counter[label] += 1
    if all(count == samples_per_class for count in class_counter.values()):
        break

unique_class_names = sorted(set(train_labels))
prompts = [f"a photo of a {name}" for name in unique_class_names]

# text_inputs = processor(text=prompts, return_tensors="tf", padding=True)


# Preprocess the 80 training images
train_inputs = processor(text=prompts, images=train_images, return_tensors="tf", padding=True)
# text_embeds = model.get_text_features(**text_inputs)
# text_embeds = tf.nn.l2_normalize(text_embeds, axis=-1)

# Compile the model (we only train the LayerNorm, so use low LR)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
loss_fn = tf.keras.losses.CosineSimilarity(axis=-1)

# Forward pass of CLIP returns image_embeds and text_embeds, use dot-product or cosine for similarity
@tf.function
def train_step(inputs):
    with tf.GradientTape() as tape:
        outputs = model(**train_inputs)
        image_embeds = outputs.image_embeds
        text_embeds = outputs.text_embeds
        text_embeds = tf.nn.l2_normalize(text_embeds, axis=-1)
        loss = -tf.reduce_mean(loss_fn(image_embeds, text_embeds))

    grads = tape.gradient(loss, trainable_vars)
    optimizer.apply_gradients(zip(grads, trainable_vars))
    return loss

# üîÅ Finetune
for epoch in range(2):
    loss = train_step(train_inputs)
    print(f"Epoch {epoch+1}, Loss: {loss.numpy():.4f}")
    print("Trainable variables:")
    for var in trainable_vars:
        print(var.name, var.shape)

# ‚úÖ Evaluation on the test set
# Let's use the remaining flower dataset as a pseudo test set
# Convert to a list and then slice

# Inference
correct = 0
for img, true_label in zip(test_images, test_labels):
    inputs = processor(images=img, return_tensors="tf", padding=True)
    image_features = model.get_image_features(**inputs)
    image_features = tf.nn.l2_normalize(image_features, axis=-1)
    sims = tf.matmul(image_features, text_features, transpose_b=True)
    pred_idx = tf.argmax(sims, axis=1).numpy()[0]
    pred_label = label_names[pred_idx]
    if pred_label == true_label:
        correct += 1

print(f"\nAccuracy on test set: {correct}/{len(test_images)} = {correct/len(test_images):.2%}")
